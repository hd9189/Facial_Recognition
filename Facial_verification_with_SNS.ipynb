{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1sA4viGoDDJMZ118VofAlJmzSx215QA2Q",
      "authorship_tag": "ABX9TyOQEdbctArSDDrSVGZIjiWa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hd9189/Facial_Recognition/blob/main/Facial_verification_with_SNS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Setup**"
      ],
      "metadata": {
        "id": "OMiRxPl3xuAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1 Install Dependencies**"
      ],
      "metadata": {
        "id": "j73VkXMpx71S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python matplotlib"
      ],
      "metadata": {
        "id": "vt7MD7MiWPA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Dependencies**"
      ],
      "metadata": {
        "id": "jNkP9iYvoZAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import standard dependencies\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "# good to work for arrays, good for tensorflow\n",
        "import numpy as np\n",
        "# can help visualize an image\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "S4cYJGGkoeO4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# import tensorflow dependencies\n",
        "# usually people use the sequential tensorflow api, but functional tensorflow api is much more flexible in terms of hardcore processing\n",
        "# building a siamanese nerual network. comparing two images and seeing if their the same\n",
        "# passing 2 images at the same time, and finding the similarity between the two images\n",
        "# importing functional api\n",
        "\n",
        "# most imported = Model(Input=[input image, verification image], Output=[1,0])\n",
        "from tensorflow.keras.models import Model\n",
        "# lots of layer types for NN\n",
        "# Layer=high level class (creating a custom layer)\n",
        "# Conv2D (standard layer) Called a convoluntional layer usually for CNN, forming convolutions\n",
        "# Dense, a fully connected layer\n",
        "# MaxPooling2D, Pool layers together and shrinks info together, taking max value overa certain region\n",
        "# Input, determines what you input into NN, input(shape=)\n",
        "# Flatten, takes all info form a layer and flattens it into more simpler\n",
        "# Import tensorflow dependencies - Functional API\n",
        "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten"
      ],
      "metadata": {
        "id": "BoitOwqWoicT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup GPU Growth**"
      ],
      "metadata": {
        "id": "w5tcNONyo11Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Avoid out of memory errors, by setting GPU memory consumption Growth\n",
        "\n",
        "# accessing all GPUs on machine\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    # setting memory growth \n",
        "    tf.config.experimental.set_memory_growth(gpu, True)"
      ],
      "metadata": {
        "id": "vQLFzY9to5nt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Folder Structures**"
      ],
      "metadata": {
        "id": "eu6nmHBhwf6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create 3 folders for data\n",
        "# anchor (real time input), positive, and negative\n",
        "\n",
        "# setup paths\n",
        "POS_PATH = os.path.join('data', \"positive\")\n",
        "NEG_PATH = os.path.join('data', 'negative')\n",
        "ANC_PATH = os.path.join('data', 'anchor')"
      ],
      "metadata": {
        "id": "0Lzvj3-tww72"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making directories\n",
        "\n",
        "# os.makedirs(POS_PATH)\n",
        "# os.makedirs(NEG_PATH)\n",
        "# os.makedirs(ANC_PATH)"
      ],
      "metadata": {
        "id": "AiTK-vnDzDwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Collect Positives and Anchors**"
      ],
      "metadata": {
        "id": "QYGO5YrP0Xkk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "\n",
        "Anchor and positive data will be collected through the webcam via opencv, while the negative data are going to be through a data set\n",
        "\n",
        "A SNS basically has 2 comparision NN, where there is a distance function determining whether or not the images when comapred to the anchor look the same.\n",
        "\n",
        "In Part two, we will be collecting the data"
      ],
      "metadata": {
        "id": "VygdCqFD17ap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Untar Labelled Faces in the Wild Dataset**"
      ],
      "metadata": {
        "id": "3KEEUa3h0dCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Uncompress TAR GZ Labelled Faces in the Wild Dataset\n",
        "\n",
        "# !tar -xf lfw.tgz"
      ],
      "metadata": {
        "id": "-JjrfuTB0btC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f51c9b32-724a-4a8d-8c6b-6c9c24da86a1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "gzip: stdin: unexpected end of file\n",
            "tar: Unexpected EOF in archive\n",
            "tar: Unexpected EOF in archive\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Move LFW Images from folder to the following respository data/negative, done by redirecting the path\n",
        "\n",
        "# # gives directory/folder name for every single folder in 'lfw' folder\n",
        "# for directory in os.listdir('lfw'):\n",
        "#   # loops through every image in every folder and get path\n",
        "#   for file in os.listdir(os.path.join('lfw', directory)):\n",
        "#     # manipulate directory\n",
        "#     EX_PATH = os.path.join('lfw', directory, file)\n",
        "#     NEW_PATH = os.path.join(NEG_PATH, file)\n",
        "#     os.replace(EX_PATH, NEW_PATH)"
      ],
      "metadata": {
        "id": "CXZ11Rvn5fZ-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Collect Positive and Anchor Classes**"
      ],
      "metadata": {
        "id": "3DGqsROc8PUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import uuid library to generate unique image names\n",
        "import uuid"
      ],
      "metadata": {
        "id": "A8VkBn54p1bG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Establish a connection to the webcam\n",
        "# cap = cv2.VideoCapture(0)\n",
        "# while cap.isOpened(): \n",
        "#     ret, frame = cap.read()\n",
        "   \n",
        "#     # Cut down frame to 250x250px, (y,x)\n",
        "#     frame = frame[200:200+250,600:600+250, :]\n",
        "    \n",
        "#     # Collect anchors when click a\n",
        "#     if cv2.waitKey(1) & 0XFF == ord('a'):\n",
        "#         # Create the unique file path and placing in folder\n",
        "#         imgname = os.path.join(ANC_PATH, f'{uuid.uuid1()}.jpg')\n",
        "#         # Write out anchor image\n",
        "#         cv2.imwrite(imgname, frame)\n",
        "    \n",
        "#     # Collect positives when click p\n",
        "#     if cv2.waitKey(1) & 0XFF == ord('p'):\n",
        "#         # Create the unique file path \n",
        "#         imgname = os.path.join(POS_PATH, f'{uuid.uuid1()}.jpg')\n",
        "#         # Write out positive image\n",
        "#         cv2.imwrite(imgname, frame)\n",
        "    \n",
        "#     # Show image back to screen\n",
        "#     cv2.imshow('Image Collection', frame)\n",
        "    \n",
        "#     # Breaking gracefully\n",
        "#     if cv2.waitKey(1) & 0XFF == ord('q'):\n",
        "#         break\n",
        "\n",
        "# # good to help forcefully stop cv2 webcam, in case of freezing\n",
        "# # Release the webcam\n",
        "# cap.release()\n",
        "# # Close the image show frame\n",
        "# cv2.destroyAllWindows"
      ],
      "metadata": {
        "id": "r_VJA_wbp2Yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print frame for testing\n",
        "# plt.imshow(frame)"
      ],
      "metadata": {
        "id": "4D24Cb6Gp5qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Load and Preprocess Images**"
      ],
      "metadata": {
        "id": "y392irkkp-W0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Get Image Directories**"
      ],
      "metadata": {
        "id": "V2NLS15BqGdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting images within directory\n",
        "# Basically get 300 files that have the PATH and the .jpg in the name\n",
        "anchor = tf.data.Dataset.list_files(ANC_PATH+'/*.jpg').take(300)\n",
        "positive = tf.data.Dataset.list_files(POS_PATH+'/*.jpg').take(300)\n",
        "negative = tf.data.Dataset.list_files(NEG_PATH+'/*.jpg').take(300)"
      ],
      "metadata": {
        "id": "rC43CLa6p7V0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(file_path):\n",
        "    # Reading image from file path\n",
        "    byte_img = tf.io.read_file(file_path)\n",
        "    # loading image\n",
        "    img = tf.io.decode_jpeg(byte_img)\n",
        "    # preprocessing steps\n",
        "    # resize image\n",
        "    img = tf.image.resize(img, (100,100))\n",
        "    # scaling image to be between 1 and 0\n",
        "    img /= 255.0 \n",
        "    return img"
      ],
      "metadata": {
        "id": "4HFizrJiqM8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Labelled Dataset**"
      ],
      "metadata": {
        "id": "Uxljt72lrXDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(anchor, positive) => [1,1,1,1,1]\n",
        "(anchor, negative) => [0,0,0,0,0]"
      ],
      "metadata": {
        "id": "2R2akGdeqWHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# .zip allows to zip everything up and iterate everything at the same time\n",
        "\n",
        "# creating a dataset using anchor, positive/negative, and an array of 1s or 0s that matches the shape of the anchor\n",
        "# saying that if we compare the anchor to positive, we want it to be 1, if we compare anchor to negative, we want it to be 0\n",
        "postives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
        "negatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
        "\n",
        "# joining positives and negatives dataset together\n",
        "data = postives.concatenate(negatives)"
      ],
      "metadata": {
        "id": "AnA1Y3fCqXfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = data.as_numpy_iterator()\n",
        "exampple = samples.next()\n",
        "exampple\n"
      ],
      "metadata": {
        "id": "EXcRWU3-qZJ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28fc2bdb-a58a-4f1d-ee4c-606e38bca82b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(b'data/anchor/1ac735e4-a911-11ed-8669-f2189828324b.jpg',\n",
              " b'data/positive/14ddf5be-a911-11ed-8669-f2189828324b.jpg',\n",
              " 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build, Train, and Test Partition**"
      ],
      "metadata": {
        "id": "y4IJWBkIrHw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_twin(input_img, validation_img, label):\n",
        "    # input_img -> anchor, validation_img -> positive/negative\n",
        "    return(preprocess(input_img), preprocess(validation_img), label)\n",
        "\n"
      ],
      "metadata": {
        "id": "Od96VPH_qaQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Dataloader pipline\n",
        "data = data.map(preprocess_twin) #applies function to the entire data set\n",
        "data = data.cache()\n",
        "data = data.shuffle(buffer_size=1024)"
      ],
      "metadata": {
        "id": "8jIDcqcfqbg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training partition\n",
        "train_data = data.take(round(len(data)*.7))\n",
        "# but 16 images in one group to train at one time\n",
        "train_data = train_data.batch(16)\n",
        "train_data = train_data.prefetch(8)"
      ],
      "metadata": {
        "id": "q3omCdMvqcSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing partition\n",
        "test_data = data.skip(round(len(data)*.7))\n",
        "test_data = test_data.take(round(len(data)*.3))\n",
        "test_data = test_data.batch(16)\n",
        "test_data = test_data.prefetch(8)"
      ],
      "metadata": {
        "id": "yNwK_j_GqdLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Model Creation**"
      ],
      "metadata": {
        "id": "ql_8FNrIqeMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embedding Layer (for NN to read image)**"
      ],
      "metadata": {
        "id": "kO5ddWQ_q9S8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inp = Input(shape=(100,100,3), name='input_image')\n",
        "c1 = Conv2D(64, (10,10), activation='relu')(inp)\n",
        "m1 = MaxPooling2D(64, (2,2), padding='same')(c1)\n",
        "c2 = Conv2D(128, (7,7), activation='relu')(m1)\n",
        "m2 = MaxPooling2D(64, (2,2), padding='same')(c2)\n",
        "c3 = Conv2D(128, (4,4), activation='relu')(m2)\n",
        "m3 = MaxPooling2D(64, (2,2), padding='same')(c3)\n",
        "c4 = Conv2D(256, (4,4), activation='relu')(m3)\n",
        "f1 = Flatten()(c4)\n",
        "d1 = Dense(4096, activation='sigmoid')(f1)\n",
        "mod = Model(inputs=[inp], outputs=[d1], name='embedding')"
      ],
      "metadata": {
        "id": "T-Mx6X2kqx78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mod.summary()"
      ],
      "metadata": {
        "id": "x9_IXAJHqzk0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c519732-3f18-4b00-99fb-5dbe645f830f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"embedding\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_image (InputLayer)     [(None, 100, 100, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 91, 91, 64)        19264     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 46, 46, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 40, 40, 128)       401536    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 20, 20, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 17, 17, 128)       262272    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 9, 9, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 6, 6, 256)         524544    \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 4096)              37752832  \n",
            "=================================================================\n",
            "Total params: 38,960,448\n",
            "Trainable params: 38,960,448\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to take input of the image\n",
        "def make_embedding(): \n",
        "    inp = Input(shape=(100,100,3), name='input_image')\n",
        "    \n",
        "    # First block\n",
        "    # reads the image, taking it apart piece by piece by pixel size.\n",
        "    # 64 10x10 parts\n",
        "    c1 = Conv2D(64, (10,10), activation='relu')(inp) # passing inp into c1\n",
        "    m1 = MaxPooling2D(64, (2,2), padding='same')(c1)\n",
        "    \n",
        "    # Second block\n",
        "    c2 = Conv2D(128, (7,7), activation='relu')(m1)\n",
        "    m2 = MaxPooling2D(64, (2,2), padding='same')(c2)\n",
        "    \n",
        "    # Third block\n",
        "    c3 = Conv2D(128, (4,4), activation='relu')(m2)\n",
        "    m3 = MaxPooling2D(64, (2,2), padding='same')(c3)\n",
        "    \n",
        "    # Final embedding block\n",
        "    c4 = Conv2D(256, (4,4), activation='relu')(m3)\n",
        "    f1 = Flatten()(c4)\n",
        "    d1 = Dense(4096, activation='sigmoid')(f1)\n",
        "    \n",
        "    \n",
        "    return Model(inputs=[inp], outputs=[d1], name='embedding')"
      ],
      "metadata": {
        "id": "W4nhjTqwq0mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = make_embedding()\n"
      ],
      "metadata": {
        "id": "UaTet_65q1yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SNN layer\n",
        "embedding.summary()\n"
      ],
      "metadata": {
        "id": "oMoPzns_q2s2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dd59774-3ffb-47fc-9356-134b665ee0e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"embedding\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_image (InputLayer)     [(None, 100, 100, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 91, 91, 64)        19264     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 46, 46, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 40, 40, 128)       401536    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling (None, 20, 20, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 17, 17, 128)       262272    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling (None, 9, 9, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 6, 6, 256)         524544    \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 4096)              37752832  \n",
            "=================================================================\n",
            "Total params: 38,960,448\n",
            "Trainable params: 38,960,448\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.2 Distance Layer**"
      ],
      "metadata": {
        "id": "AYY5p_8oq4im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Siamese L1 Distance class\n",
        "# Sort of like a custom NN layer\n",
        "class L1Dist(Layer):\n",
        "    \n",
        "    # Init method - inheritance\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "       \n",
        "    # Magic happens here - similarity calculation\n",
        "    def call(self, input_embedding, validation_embedding):\n",
        "        return tf.math.abs(input_embedding - validation_embedding)"
      ],
      "metadata": {
        "id": "yoIvg5iCq7CN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Siamese Model**"
      ],
      "metadata": {
        "id": "rFoh7UdJrth5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_image = Input(name='input_img', shape=(100,100,3))\n",
        "validation_image = Input(name='validation_img', shape=(100,100,3))"
      ],
      "metadata": {
        "id": "8o2kkeLbrxN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp_embedding = embedding(input_image)\n",
        "val_embedding = embedding(validation_image)"
      ],
      "metadata": {
        "id": "7yF-MOsgrysI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_layer = L1Dist()"
      ],
      "metadata": {
        "id": "dW8JUPbxrzsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distances = siamese_layer(inp_embedding, val_embedding)"
      ],
      "metadata": {
        "id": "3--flo-gr0km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = Dense(1, activation='sigmoid')(distances)"
      ],
      "metadata": {
        "id": "KSp2IQvZr1pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_network = Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')"
      ],
      "metadata": {
        "id": "rUUoHi6Mr28F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_siamese_model(): \n",
        "    \n",
        "    # Anchor image input in the network\n",
        "    input_image = Input(name='input_img', shape=(100,100,3))\n",
        "    \n",
        "    # Validation image in the network \n",
        "    validation_image = Input(name='validation_img', shape=(100,100,3))\n",
        "    \n",
        "    # Combine siamese distance components\n",
        "    siamese_layer = L1Dist()\n",
        "    siamese_layer._name = 'distance'\n",
        "    distances = siamese_layer(embedding(input_image), embedding(validation_image))\n",
        "    \n",
        "    # Classification layer, final output, 1 or 0\n",
        "    classifier = Dense(1, activation='sigmoid')(distances)\n",
        "    \n",
        "    # All the layers together\n",
        "    return Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')"
      ],
      "metadata": {
        "id": "GyqE1dEsr6GB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_model = make_siamese_model()"
      ],
      "metadata": {
        "id": "KfGoLkQ_r7PH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_model.summary()"
      ],
      "metadata": {
        "id": "ugui8k1Ar8Io",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b348479-e46a-4b70-a216-7e219f06a1c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"SiameseNetwork\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_img (InputLayer)          [(None, 100, 100, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "validation_img (InputLayer)     [(None, 100, 100, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Functional)          (None, 4096)         38960448    input_img[0][0]                  \n",
            "                                                                 validation_img[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "distance (L1Dist)               (None, 4096)         0           embedding[2][0]                  \n",
            "                                                                 embedding[3][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 1)            4097        distance[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 38,964,545\n",
            "Trainable params: 38,964,545\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Training**"
      ],
      "metadata": {
        "id": "9vn-DRjjr87h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic flow for training on one batch:\n",
        "1. make prediction\n",
        "2. Calculate loss\n",
        "3. Derive gradeints\n",
        "4. Calculate new weights and apply"
      ],
      "metadata": {
        "id": "fAAP5hwVsDi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting up Loss and Optimizer**"
      ],
      "metadata": {
        "id": "1VZj4iaEsFlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "binary_cross_loss = tf.losses.BinaryCrossentropy()"
      ],
      "metadata": {
        "id": "rA-TIZg6sEpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# learning rate of 1e-4 --> 0.0001\n",
        "opt = tf.keras.optimizers.Adam(1e-4)"
      ],
      "metadata": {
        "id": "BKF698IKsBK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Establish Checkpoints**"
      ],
      "metadata": {
        "id": "a_chlIoWsN9m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If something goes wrong during training, we can go back to these checkpoints"
      ],
      "metadata": {
        "id": "vCWKyVmFsX8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note to self: To reload from the checkpoint, you can use model.load('path_to_checkpoint').\n",
        "this will load the pre trained weights into existing model"
      ],
      "metadata": {
        "id": "OOaxuTnrsUC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create directory for training checkpoints\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "\n",
        "# creastion checkpoint prefix, all start with ckpt\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(opt=opt, siamese_model=siamese_model)\n"
      ],
      "metadata": {
        "id": "rOFyOeFasZ6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build, Train, Step Function**"
      ],
      "metadata": {
        "id": "rgzfn_Nysbc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each batch in the dataset is comprised of 16 samples, each of which contians an anchor image, a pos or neg image, and a label\n"
      ],
      "metadata": {
        "id": "5CHHAWLtsqvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# decorator to complie a function into a callable tf graph\n",
        "# make the entire NN into a graph to train more efficiently\n",
        "@tf.function\n",
        "def train_step(batch):\n",
        "\n",
        "    # record operations for automatic differentiation\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        # Get anchor and pos/neg image (features)\n",
        "        X = batch[:2]\n",
        "\n",
        "        # Get label\n",
        "        y = batch[2]\n",
        "\n",
        "        # Forward pass\n",
        "        # NOTE: important to have traning=True as some layers only activate when this happens\n",
        "        ypred = siamese_model(X, training=True)\n",
        "        # Calculate loss, passing through y=true, ypred=predicted outcome\n",
        "        loss = binary_cross_loss(y ,ypred)\n",
        "    print(loss)\n",
        "    # Calculate gradients\n",
        "    grad = tape.gradient(loss, siamese_model.trainable_variables)\n",
        "\n",
        "    #Calculate updated wights and apply to siamese model\n",
        "    opt.apply_gradients(zip(grad, siamese_model.trainable_variables))\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "CziR4jlBsrpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Loop**"
      ],
      "metadata": {
        "id": "txJ_dzbZstVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: train_step function focused on training one batch, and this loop will be used to iterate through every batch in the dataset\n",
        "def train(data, EPOCHS):\n",
        "    # Loop through epochs\n",
        "    for epoch in range(1, EPOCHS+1):\n",
        "\n",
        "        # indicate what epoch at\n",
        "        print(f'\\n Epoch {epoch}/{EPOCHS}')\n",
        "\n",
        "        # defining progress bar to show it during training\n",
        "        progbar = tf.keras.utils.Progbar(len(data))\n",
        "    # Loop through each batch\n",
        "        for index, batch in enumerate(data):\n",
        "        # Run train step\n",
        "            train_step(batch)\n",
        "            progbar.update(index+1)\n",
        "\n",
        "    # Save checkpoints\n",
        "    if epoch % 10 ==0:\n",
        "        checkpoint.save(file_prefix=checkpoint_prefix)"
      ],
      "metadata": {
        "id": "LnfZDGl6sx1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Model**"
      ],
      "metadata": {
        "id": "yFAmFnYgszff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# number of training datasets\n",
        "EPOCHS = 50"
      ],
      "metadata": {
        "id": "TjC7HbvYs1wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_data, EPOCHS)"
      ],
      "metadata": {
        "id": "Y4XqNmM5s3kU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "outputId": "c04f5648-486c-4bc6-b19d-ed49fc8936fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1/50\n",
            "Tensor(\"binary_crossentropy/weighted_loss/value:0\", shape=(), dtype=float32)\n",
            "Tensor(\"binary_crossentropy/weighted_loss/value:0\", shape=(), dtype=float32)\n",
            "7/8 [=========================>....] - ETA: 21sTensor(\"binary_crossentropy/weighted_loss/value:0\", shape=(), dtype=float32)\n",
            "8/8 [==============================] - 158s 19s/step\n",
            "\n",
            " Epoch 2/50\n",
            "8/8 [==============================] - 149s 18s/step\n",
            "\n",
            " Epoch 3/50\n",
            "8/8 [==============================] - 148s 18s/step\n",
            "\n",
            " Epoch 4/50\n",
            "8/8 [==============================] - 150s 18s/step\n",
            "\n",
            " Epoch 5/50\n",
            "8/8 [==============================] - 149s 18s/step\n",
            "\n",
            " Epoch 6/50\n",
            "8/8 [==============================] - 149s 18s/step\n",
            "\n",
            " Epoch 7/50\n",
            "8/8 [==============================] - 149s 18s/step\n",
            "\n",
            " Epoch 8/50\n",
            "5/8 [=================>............] - ETA: 1:04"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-148-3a6c1daed93b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-146-bc9e5e47ed45>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data, EPOCHS)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Run train step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}